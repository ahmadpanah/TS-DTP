{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "REZDbYRPui6U",
        "outputId": "02fc0004-f7d2-4b6f-c4f0-9c33af577185"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The expanded size of the tensor (64) must match the existing size (11) at non-singleton dimension 2.  Target sizes: [1, 11, 64].  Tensor sizes: [11, 11]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-f3ef3ec278fa>\u001b[0m in \u001b[0;36m<cell line: 143>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m   \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m   \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts_dtp_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-f3ef3ec278fa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m    118\u001b[0m          \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transformer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m          \u001b[0mtoken_importance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_token_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m          \u001b[0mpruned_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_importance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-f3ef3ec278fa>\u001b[0m in \u001b[0;36mcalculate_token_importance\u001b[0;34m(self, hidden_states, inputs_embeds, attention_mask)\u001b[0m\n\u001b[1;32m     67\u001b[0m                     \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0mattention_weights_per_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                     \u001b[0mattention_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_weights_per_head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                     \u001b[0;31m#attention_weights[:, head_idx, :] = attention_weights_per_head.view(batch_size, seq_len, layer_output.shape[-1] // self.model.config.num_attention_heads)  # Adjust target shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (64) must match the existing size (11) at non-singleton dimension 2.  Target sizes: [1, 11, 64].  Tensor sizes: [11, 11]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class TaskSpecificAttention(nn.Module):\n",
        "    def __init__(self, config, task_feature_dim):\n",
        "        super().__init__()\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
        "        self.task_specific_weight = nn.Parameter(torch.randn(config.hidden_size, task_feature_dim))\n",
        "        self.feature_layer = nn.Linear(config.hidden_size, task_feature_dim)\n",
        "\n",
        "    def forward(self, hidden_states, inputs_embeds):\n",
        "        \"\"\"\n",
        "        :param hidden_states: output of transformer layers (batch_size, seq_len, hidden_size)\n",
        "        :param inputs_embeds: input embeddings (batch_size, seq_len, hidden_size)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
        "\n",
        "        # Transform hidden states to get task-specific features\n",
        "        task_features = self.feature_layer(inputs_embeds) # (batch_size, seq_len, task_feature_dim)\n",
        "\n",
        "        # Calculate task-specific attention\n",
        "        task_attn_weights = torch.matmul(task_features, self.task_specific_weight.t())  # (batch_size, seq_len, hidden_size)\n",
        "        task_attn_weights = F.softmax(task_attn_weights, dim=-1)\n",
        "\n",
        "        # Reshape for combining\n",
        "        task_attn_weights = task_attn_weights.reshape(batch_size, seq_len, self.num_heads, self.head_dim) # (batch_size, seq_len, num_heads, head_dim)\n",
        "        task_attn_weights = task_attn_weights.transpose(1, 2) # (batch_size, num_heads, seq_len, head_dim)\n",
        "\n",
        "        return task_attn_weights\n",
        "\n",
        "class TaskSpecificDynamicTokenPruning(nn.Module):\n",
        "    def __init__(self, model_name, task_feature_dim, gamma=0.5, lambda_aux=0.1):\n",
        "        super().__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.num_layers = len(self.model.encoder.layer) if hasattr(self.model, 'encoder') else len(self.model.transformer.layer)\n",
        "        self.task_attn = nn.ModuleList([TaskSpecificAttention(self.model.config, task_feature_dim) for _ in range(self.num_layers)])\n",
        "        self.gamma = gamma\n",
        "        self.lambda_aux = lambda_aux\n",
        "\n",
        "    def calculate_token_importance(self, hidden_states, inputs_embeds, attention_mask):\n",
        "        \"\"\"\n",
        "        :param hidden_states: output of transformer layers (list of (batch_size, seq_len, hidden_size))\n",
        "        :param inputs_embeds: input embeddings (batch_size, seq_len, hidden_size)\n",
        "        :param attention_mask: attention mask for the tokens (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        device = hidden_states[0].device\n",
        "        batch_size, seq_len, _ = hidden_states[0].shape\n",
        "        token_importance = torch.zeros(batch_size, seq_len, device=device)\n",
        "\n",
        "        for layer_idx, layer_output in enumerate(hidden_states):\n",
        "            layer_output = layer_output.to(device)\n",
        "            # Calculate Standard Attention Weights for each layer\n",
        "            attention_weights = torch.zeros(batch_size, self.model.config.num_attention_heads, seq_len, layer_output.shape[-1]//self.model.config.num_attention_heads, device=device)\n",
        "            if hasattr(self.model, 'encoder'):  # BERT and similar models\n",
        "                for head_idx, head in enumerate(self.model.encoder.layer[layer_idx].attention.self.key.weight):\n",
        "                    q = self.model.encoder.layer[layer_idx].attention.self.query(layer_output)\n",
        "                    k = self.model.encoder.layer[layer_idx].attention.self.key(layer_output)\n",
        "                    # Reshape attention_weights to match softmax output\n",
        "                    #The original code tried to reshape a tensor with 704 elements to (1,11,11) which has 121 elements.\n",
        "                    #Instead, we calculate the correct dimensions based on the size of the tensor, enabling successful reshaping.\n",
        "                    attention_weights_per_head = attention_weights[:, head_idx, :].view(batch_size, seq_len, layer_output.shape[-1] // self.model.config.num_attention_heads) # Adjust target shape\n",
        "\n",
        "                    attention_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(layer_output.shape[-1], dtype=torch.float32, device=device))\n",
        "                    attention_weights_per_head = torch.softmax(attention_scores, dim=-1)\n",
        "                    attention_weights[:, head_idx, :] = attention_weights_per_head\n",
        "                    #attention_weights[:, head_idx, :] = attention_weights_per_head.view(batch_size, seq_len, layer_output.shape[-1] // self.model.config.num_attention_heads)  # Adjust target shape\n",
        "\n",
        "            elif hasattr(self.model, 'transformer'): # GPT and similar models\n",
        "                for head_idx, head in enumerate(self.model.transformer.layer[layer_idx].attn.c_attn.weight.transpose(0,-1)[0]):\n",
        "                    q = self.model.transformer.layer[layer_idx].attn.c_attn(layer_output)  # (batch_size, seq_len, hidden_size)\n",
        "                    k = torch.matmul(layer_output, head)  # (batch_size, seq_len, hidden_size)\n",
        "                    attention_weights[:, head_idx, :] = torch.softmax(torch.matmul(q,k.transpose(-2,-1))/torch.sqrt(torch.tensor(layer_output.shape[-1], dtype=torch.float32, device=device)), dim = -1)\n",
        "\n",
        "\n",
        "            # Task specific attention\n",
        "            task_attn_weights = self.task_attn[layer_idx](hidden_states=layer_output, inputs_embeds=inputs_embeds) # (batch_size, num_heads, seq_len, head_dim)\n",
        "            hybrid_attention_weights = attention_weights + self.gamma* task_attn_weights\n",
        "\n",
        "            # Calculate Average importance\n",
        "            layer_importance = torch.sum(hybrid_attention_weights, dim=(1,3)) # (batch_size, seq_len)\n",
        "            #layer_importance = torch.mean(layer_importance, dim=1) # (batch_size, seq_len) # Not needed anymore due to task-specific attention\n",
        "            token_importance += layer_importance\n",
        "\n",
        "        return token_importance # (batch_size, seq_len)\n",
        "\n",
        "    def prune_tokens(self, token_importance, attention_mask):\n",
        "         \"\"\"\n",
        "        :param token_importance: token importance scores (batch_size, seq_len)\n",
        "        :param attention_mask: attention mask for the tokens (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "         batch_size, seq_len = token_importance.shape\n",
        "         mask_output = attention_mask.clone()\n",
        "\n",
        "         threshold = torch.mean(token_importance, dim=1, keepdim=True) - torch.std(token_importance, dim=1, keepdim=True) # (batch_size, 1)\n",
        "         pruned_mask = token_importance < threshold\n",
        "         mask_output = mask_output.masked_fill(pruned_mask, 0)\n",
        "\n",
        "         return mask_output # (batch_size, seq_len)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "         \"\"\"\n",
        "         :param input_ids: input ids (batch_size, seq_len)\n",
        "         :param attention_mask: attention mask for the tokens (batch_size, seq_len)\n",
        "         :param labels: (optional) labels for the downstream task (batch_size, num_labels)\n",
        "         \"\"\"\n",
        "         device = input_ids.device\n",
        "         inputs_embeds = self.model.embeddings(input_ids) # (batch_size, seq_len, hidden_size)\n",
        "         hidden_states = []\n",
        "         output = self.model(input_ids=None, inputs_embeds=inputs_embeds, attention_mask=attention_mask, output_hidden_states=True)\n",
        "         if hasattr(self.model, 'encoder'):\n",
        "             hidden_states = output.hidden_states[1:] # Remove the embedding output\n",
        "         elif hasattr(self.model, 'transformer'):\n",
        "            hidden_states = output.hidden_states[1:]\n",
        "         token_importance = self.calculate_token_importance(hidden_states, inputs_embeds, attention_mask)\n",
        "         pruned_attention_mask = self.prune_tokens(token_importance, attention_mask)\n",
        "\n",
        "         output = self.model(input_ids=None, inputs_embeds=inputs_embeds, attention_mask = pruned_attention_mask, labels=labels)\n",
        "         loss = output.loss\n",
        "         logits = output.logits\n",
        "\n",
        "         if labels is not None:\n",
        "             aux_loss = self.calculate_auxiliary_loss(token_importance) # auxiliary loss function\n",
        "             total_loss = loss + self.lambda_aux * aux_loss\n",
        "             return total_loss, logits\n",
        "\n",
        "         else:\n",
        "             return logits\n",
        "\n",
        "    def calculate_auxiliary_loss(self, token_importance):\n",
        "       \"\"\"\n",
        "        :param token_importance: token importance scores (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "       batch_size, seq_len = token_importance.shape\n",
        "       aux_loss = torch.mean(torch.abs(torch.mean(token_importance, dim=1, keepdim=True) - torch.mean(token_importance)))\n",
        "       return aux_loss\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # Example Usage\n",
        "  model_name = \"bert-base-uncased\"  # Or any other suitable model\n",
        "  task_feature_dim = 128 # Define size of the task-specific feature vector\n",
        "  ts_dtp_model = TaskSpecificDynamicTokenPruning(model_name, task_feature_dim)\n",
        "\n",
        "\n",
        "  tokenizer = ts_dtp_model.tokenizer\n",
        "  text = \"This is an example input text for the test\"\n",
        "  labels = torch.tensor([1]) #Example usage for classification task\n",
        "\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "  input_ids = inputs[\"input_ids\"]\n",
        "  attention_mask = inputs[\"attention_mask\"]\n",
        "  output = ts_dtp_model(input_ids, attention_mask, labels)\n",
        "  if labels is not None:\n",
        "        loss, logits = output\n",
        "        print(\"Loss:\", loss)\n",
        "        print(\"Logits\", logits)\n",
        "  else:\n",
        "      logits = output\n",
        "      print(\"Logits:\", logits)"
      ]
    }
  ]
}