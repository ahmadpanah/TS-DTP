{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REZDbYRPui6U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "\n",
        "class TaskSpecificAttention(nn.Module):\n",
        "    def __init__(self, config, task_feature_dim):\n",
        "        super().__init__()\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
        "        self.task_specific_weight = nn.Parameter(torch.randn(config.hidden_size, task_feature_dim))\n",
        "        self.feature_layer = nn.Linear(config.hidden_size, task_feature_dim)\n",
        "\n",
        "    def forward(self, hidden_states, inputs_embeds):\n",
        "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
        "\n",
        "        # Transform hidden states to get task-specific features\n",
        "        task_features = self.feature_layer(inputs_embeds)  # (batch_size, seq_len, task_feature_dim)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        attention_scores = torch.matmul(task_features, task_features.transpose(-2, -1))\n",
        "        attention_scores = attention_scores / math.sqrt(task_features.size(-1))\n",
        "\n",
        "        # Apply softmax to get attention weights - shape: (batch_size, seq_len, seq_len)\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # Expand for multiple heads - shape: (batch_size, num_heads, seq_len, seq_len)\n",
        "        attention_weights = attention_weights.unsqueeze(1).expand(-1, self.num_heads, -1, -1)\n",
        "\n",
        "        return attention_weights\n",
        "\n",
        "class TaskSpecificDynamicTokenPruning(nn.Module):\n",
        "    def __init__(self, model_name, task_feature_dim, num_labels=2, gamma=0.5, lambda_aux=0.1):\n",
        "        super().__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "        self.num_layers = len(self.model.bert.encoder.layer)\n",
        "        self.task_attn = nn.ModuleList([TaskSpecificAttention(self.model.config, task_feature_dim)\n",
        "                                       for _ in range(self.num_layers)])\n",
        "        self.gamma = gamma\n",
        "        self.lambda_aux = lambda_aux\n",
        "        self.classifier = nn.Linear(self.model.config.hidden_size, num_labels)\n",
        "\n",
        "    def calculate_token_importance(self, hidden_states, inputs_embeds, attention_mask):\n",
        "        device = hidden_states[0].device\n",
        "        batch_size, seq_len, _ = hidden_states[0].shape\n",
        "        token_importance = torch.zeros(batch_size, seq_len, device=device)\n",
        "\n",
        "        for layer_idx, layer_output in enumerate(hidden_states):\n",
        "            # Standard attention weights\n",
        "            q = self.model.bert.encoder.layer[layer_idx].attention.self.query(layer_output)\n",
        "            k = self.model.bert.encoder.layer[layer_idx].attention.self.key(layer_output)\n",
        "\n",
        "            # Reshape for multi-head attention\n",
        "            head_dim = self.model.config.hidden_size // self.model.config.num_attention_heads\n",
        "            q = q.view(batch_size, seq_len, self.model.config.num_attention_heads, head_dim).transpose(1, 2)\n",
        "            k = k.view(batch_size, seq_len, self.model.config.num_attention_heads, head_dim).transpose(1, 2)\n",
        "\n",
        "            # Calculate attention scores\n",
        "            attention_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(head_dim)\n",
        "            attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "            # Task-specific attention\n",
        "            task_attn_weights = self.task_attn[layer_idx](layer_output, inputs_embeds)\n",
        "\n",
        "            # Combine attention weights\n",
        "            hybrid_attention_weights = attention_weights + self.gamma * task_attn_weights\n",
        "\n",
        "            # Calculate importance\n",
        "            layer_importance = torch.mean(hybrid_attention_weights, dim=1).mean(dim=-1)\n",
        "            token_importance += layer_importance\n",
        "\n",
        "        return token_importance\n",
        "\n",
        "    def prune_tokens(self, token_importance, attention_mask):\n",
        "        mask_output = attention_mask.clone()\n",
        "        threshold = torch.mean(token_importance, dim=1, keepdim=True) - torch.std(token_importance, dim=1, keepdim=True)\n",
        "        pruned_mask = token_importance < threshold\n",
        "        mask_output = mask_output.masked_fill(pruned_mask, 0)\n",
        "        return mask_output\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        device = input_ids.device\n",
        "\n",
        "        # First pass through BERT\n",
        "        outputs = self.model.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs.hidden_states[1:]  # Skip embedding layer\n",
        "        inputs_embeds = self.model.bert.embeddings(input_ids)\n",
        "\n",
        "        # Calculate token importance and prune\n",
        "        token_importance = self.calculate_token_importance(hidden_states, inputs_embeds, attention_mask)\n",
        "        pruned_attention_mask = self.prune_tokens(token_importance, attention_mask)\n",
        "\n",
        "        # Second pass with pruned attention mask\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=pruned_attention_mask,\n",
        "            labels=labels,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = outputs.loss\n",
        "            aux_loss = self.calculate_auxiliary_loss(token_importance)\n",
        "            total_loss = loss + self.lambda_aux * aux_loss\n",
        "            return total_loss, outputs.logits\n",
        "        return outputs.logits\n",
        "\n",
        "    def calculate_auxiliary_loss(self, token_importance):\n",
        "        return torch.mean(torch.abs(\n",
        "            torch.mean(token_importance, dim=1, keepdim=True) - torch.mean(token_importance)\n",
        "        ))\n",
        "\n",
        "# Example usage\n",
        "if __name__ == '__main__':\n",
        "    model_name = \"bert-base-uncased\"\n",
        "    task_feature_dim = 128\n",
        "    num_labels = 2\n",
        "\n",
        "    # Initialize model\n",
        "    ts_dtp_model = TaskSpecificDynamicTokenPruning(model_name, task_feature_dim, num_labels)\n",
        "\n",
        "    # Move model to GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    ts_dtp_model = ts_dtp_model.to(device)\n",
        "\n",
        "    # Prepare input\n",
        "    text = \"This is an example input text for the test\"\n",
        "    labels = torch.tensor([1]).to(device)\n",
        "\n",
        "    # Tokenize and process\n",
        "    inputs = ts_dtp_model.tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    )\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    output = ts_dtp_model(input_ids, attention_mask, labels)\n",
        "\n",
        "    if labels is not None:\n",
        "        loss, logits = output\n",
        "        print(\"Loss:\", loss.item())\n",
        "        print(\"Logits:\", logits)\n",
        "    else:\n",
        "        logits = output\n",
        "        print(\"Logits:\", logits)"
      ]
    }
  ]
}
